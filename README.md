# nrrubSearch ğŸ”

Un moteur de recherche moderne et personnalisable avec interface utilisateur avancÃ©e.

## ğŸš€ FonctionnalitÃ©s

- **Interface moderne** : Design responsive avec mode sombre/clair
- **Recherche avancÃ©e** : Filtres par type de contenu, tri, limitation des rÃ©sultats
- **Suggestions intelligentes** : AutocomplÃ©tion basÃ©e sur l'historique
- **Historique de recherche** : Sauvegarde locale des recherches prÃ©cÃ©dentes
- **API REST** : Backend FastAPI avec endpoints de recherche

## ğŸ“ Structure du projet

```
nrrubSearch/
â”œâ”€â”€ backend/           # API FastAPI
â”‚   â”œâ”€â”€ main.py       # Serveur principal
â”‚   â”œâ”€â”€ crawler.py    # Collecteur de donnÃ©es
â”‚   â”œâ”€â”€ indexer.py    # CrÃ©ation d'index
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/         # Interface utilisateur
â”‚   â”œâ”€â”€ script.js     # Logique principale
â”‚   â”œâ”€â”€ style.css     # Styles CSS
â”‚   â””â”€â”€ *.html        # Pages de l'interface
â”œâ”€â”€ assets/           # Ressources (logos, icÃ´nes)
â””â”€â”€ vercel.json       # Configuration de dÃ©ploiement
```

## ğŸ› ï¸ Installation et dÃ©veloppement local

### PrÃ©requis
- Python 3.8+
- Node.js (optionnel, pour le dÃ©veloppement frontend)

### 1. Cloner le projet
```bash
git clone <votre-repo>
cd nrrubSearch
```

### 2. Installer les dÃ©pendances backend
```bash
cd backend
pip install -r requirements.txt
```

### 3. GÃ©nÃ©rer l'index de recherche
```bash
# Collecter les donnÃ©es (optionnel, des donnÃ©es d'exemple existent dÃ©jÃ )
python crawler.py

# CrÃ©er l'index de recherche
python indexer.py
```

### 4. DÃ©marrer le serveur de dÃ©veloppement
```bash
# DÃ©marrer le backend sur le port 8002
uvicorn main:app --reload --port 8002

# Dans un autre terminal, servir le frontend
# Option 1: Serveur Python simple
python -m http.server 3000

# Option 2: Live Server (VS Code extension)
# Ouvrir index.html avec Live Server
```

### 5. AccÃ©der Ã  l'application
- Frontend : http://localhost:3000
- API Backend : http://localhost:8002/api/health

## ğŸŒ DÃ©ploiement sur Vercel

### MÃ©thode 1 : Via l'interface Vercel
1. Connectez votre repository GitHub Ã  Vercel
2. Vercel dÃ©tectera automatiquement la configuration `vercel.json`
3. Le dÃ©ploiement se fera automatiquement

### MÃ©thode 2 : Via CLI Vercel
```bash
# Installer Vercel CLI
npm i -g vercel

# Se connecter Ã  Vercel
vercel login

# DÃ©ployer le projet
vercel --prod
```

### Variables d'environnement (optionnel)
Pour la production, vous pouvez configurer :
- `CORS_ORIGINS` : Domaines autorisÃ©s pour CORS
- `INDEX_FILE_PATH` : Chemin vers le fichier d'index

## ğŸ“¡ API Endpoints

### GET /api/search
Recherche dans l'index

**ParamÃ¨tres :**
- `q` (string) : Terme de recherche

**RÃ©ponse :**
```json
[
  {
    "title": "Titre de la page",
    "url": "URL ou chemin du fichier"
  }
]
```

### GET /api/health
VÃ©rification de l'Ã©tat du serveur

**RÃ©ponse :**
```json
{
  "status": "ok",
  "index_loaded": true
}
```

## ğŸ”§ Personnalisation

### Ajouter de nouvelles sources
1. Modifier `crawler.py` pour ajouter des URLs
2. ExÃ©cuter le crawler : `python crawler.py`
3. RÃ©gÃ©nÃ©rer l'index : `python indexer.py`
4. RedÃ©marrer le serveur

### Modifier l'interface
- **Styles** : Ã‰diter `frontend/style.css`
- **FonctionnalitÃ©s** : Modifier `frontend/script.js`
- **Pages** : Ajouter des fichiers HTML dans `frontend/`

## ğŸ› RÃ©solution de problÃ¨mes

### Erreur "VÃ©rifiez la connexion au serveur"
- VÃ©rifiez que le backend est dÃ©marrÃ©
- ContrÃ´lez l'URL de l'API dans la console du navigateur
- En local : backend doit Ãªtre sur le port 8002

### Index vide ou recherche sans rÃ©sultats
- ExÃ©cutez `python indexer.py` dans le dossier backend
- VÃ©rifiez que `index.json` existe et contient des donnÃ©es

### ProblÃ¨mes de CORS en dÃ©veloppement
- Le middleware CORS est configurÃ© pour accepter toutes les origines
- En production, modifiez `allow_origins` dans `main.py`

## ğŸš€ AmÃ©liorations futures

- [ ] Base de donnÃ©es persistante (PostgreSQL/MongoDB)
- [ ] Scoring TF-IDF pour la pertinence
- [ ] Authentification utilisateur
- [ ] Crawling automatique et planifiÃ©
- [ ] Analytics et monitoring
- [ ] Support de fichiers PDF/DOC
- [ ] Recherche par facettes
- [ ] API de suggestion avancÃ©e

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier LICENSE pour plus de dÃ©tails.

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une issue ou soumettre une pull request.

---

**DÃ©veloppÃ© avec â¤ï¸ en utilisant FastAPI et JavaScript vanilla**